{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Threshold Tuning and Model Explainability\n",
                "## Transaction Fraud Detection System\n",
                "\n",
                "This notebook covers:\n",
                "1. Threshold optimization for business constraints\n",
                "2. Precision-Recall trade-off analysis\n",
                "3. SHAP explainability (global and local)\n",
                "4. Generate fraud score predictions\n",
                "5. Production recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import sys\n",
                "sys.path.append('../src')\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import lightgbm as lgb\n",
                "import shap\n",
                "\n",
                "from data_loader import load_raw_data, clean_data\n",
                "from features import engineer_features, get_feature_columns\n",
                "from train import time_based_split, load_model\n",
                "from evaluate import precision_at_k, recall_at_k\n",
                "from sklearn.metrics import precision_recall_curve\n",
                "\n",
                "# Set plotting style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Initialize SHAP\n",
                "shap.initjs()\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data and Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and prepare data\n",
                "df = load_raw_data(nrows=500000)  # Adjust based on your system\n",
                "df_clean = clean_data(df)\n",
                "df_featured, _ = engineer_features(df_clean)\n",
                "\n",
                "# Get features\n",
                "feature_cols = get_feature_columns(df_featured)\n",
                "\n",
                "# Time-based split\n",
                "train_df, test_df = time_based_split(df_featured, test_size=0.2)\n",
                "\n",
                "X_train = train_df[feature_cols]\n",
                "y_train = train_df['isFraud']\n",
                "X_test = test_df[feature_cols]\n",
                "y_test = test_df['isFraud']\n",
                "\n",
                "print(f\"Test set: {len(X_test):,} transactions\")\n",
                "print(f\"Test fraud rate: {y_test.mean()*100:.4f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load trained model\n",
                "# If model file doesn't exist, train a new one\n",
                "try:\n",
                "    model = load_model('../models/lightgbm_best.txt', model_type='lightgbm')\n",
                "    print(\"Loaded existing model\")\n",
                "except:\n",
                "    print(\"Model not found. Training new model...\")\n",
                "    from train import train_lightgbm\n",
                "    model = train_lightgbm(X_train, y_train, X_test, y_test)\n",
                "    from train import save_model\n",
                "    save_model(model, '../models/lightgbm_best.txt', model_type='lightgbm')\n",
                "\n",
                "# Get predictions\n",
                "y_pred_proba = model.predict(X_test)\n",
                "print(f\"\\nPredictions generated for {len(y_pred_proba):,} transactions\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Threshold Optimization\n",
                "\n",
                "Finding the optimal threshold based on business constraints."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate precision and recall for different thresholds\n",
                "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
                "\n",
                "# Calculate F1 score for each threshold\n",
                "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
                "\n",
                "# Find optimal threshold based on different criteria\n",
                "# 1. Maximum F1 score\n",
                "optimal_idx_f1 = np.argmax(f1_scores)\n",
                "optimal_threshold_f1 = thresholds[optimal_idx_f1] if optimal_idx_f1 < len(thresholds) else 0.5\n",
                "\n",
                "# 2. Precision >= 0.9 (high confidence)\n",
                "high_precision_idx = np.where(precision >= 0.9)[0]\n",
                "if len(high_precision_idx) > 0:\n",
                "    optimal_threshold_precision = thresholds[high_precision_idx[0]] if high_precision_idx[0] < len(thresholds) else 0.9\n",
                "else:\n",
                "    optimal_threshold_precision = 0.9\n",
                "\n",
                "# 3. Recall >= 0.7 (catch most frauds)\n",
                "high_recall_idx = np.where(recall >= 0.7)[0]\n",
                "if len(high_recall_idx) > 0:\n",
                "    optimal_threshold_recall = thresholds[high_recall_idx[-1]] if high_recall_idx[-1] < len(thresholds) else 0.3\n",
                "else:\n",
                "    optimal_threshold_recall = 0.3\n",
                "\n",
                "print(\"Optimal Thresholds:\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Maximum F1 Score:        {optimal_threshold_f1:.4f} (F1={f1_scores[optimal_idx_f1]:.4f})\")\n",
                "print(f\"High Precision (≥0.9):   {optimal_threshold_precision:.4f}\")\n",
                "print(f\"High Recall (≥0.7):      {optimal_threshold_recall:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Precision-Recall trade-off\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Precision-Recall curve with thresholds\n",
                "axes[0].plot(recall, precision, linewidth=2, color='steelblue')\n",
                "axes[0].scatter([recall[optimal_idx_f1]], [precision[optimal_idx_f1]], \n",
                "                color='red', s=100, zorder=5, label=f'Max F1 (threshold={optimal_threshold_f1:.3f})')\n",
                "axes[0].axhline(y=0.9, color='green', linestyle='--', alpha=0.5, label='Precision=0.9')\n",
                "axes[0].axvline(x=0.7, color='orange', linestyle='--', alpha=0.5, label='Recall=0.7')\n",
                "axes[0].set_xlabel('Recall', fontsize=12)\n",
                "axes[0].set_ylabel('Precision', fontsize=12)\n",
                "axes[0].set_title('Precision-Recall Curve with Optimal Threshold', fontsize=14, fontweight='bold')\n",
                "axes[0].legend(loc='best')\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# F1 score vs threshold\n",
                "axes[1].plot(thresholds, f1_scores[:-1], linewidth=2, color='purple')\n",
                "axes[1].scatter([optimal_threshold_f1], [f1_scores[optimal_idx_f1]], \n",
                "                color='red', s=100, zorder=5, label=f'Max F1={f1_scores[optimal_idx_f1]:.3f}')\n",
                "axes[1].set_xlabel('Threshold', fontsize=12)\n",
                "axes[1].set_ylabel('F1 Score', fontsize=12)\n",
                "axes[1].set_title('F1 Score vs Threshold', fontsize=14, fontweight='bold')\n",
                "axes[1].legend(loc='best')\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../outputs/threshold_optimization.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Business Scenario Analysis\n",
                "\n",
                "Different thresholds for different business needs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze different threshold scenarios\n",
                "scenarios = {\n",
                "    'Conservative (High Precision)': 0.9,\n",
                "    'Balanced (Max F1)': optimal_threshold_f1,\n",
                "    'Aggressive (High Recall)': 0.3,\n",
                "    'Default': 0.5\n",
                "}\n",
                "\n",
                "scenario_results = []\n",
                "\n",
                "for scenario_name, threshold in scenarios.items():\n",
                "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
                "    \n",
                "    tp = ((y_pred == 1) & (y_test == 1)).sum()\n",
                "    fp = ((y_pred == 1) & (y_test == 0)).sum()\n",
                "    fn = ((y_pred == 0) & (y_test == 1)).sum()\n",
                "    tn = ((y_pred == 0) & (y_test == 0)).sum()\n",
                "    \n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    \n",
                "    scenario_results.append({\n",
                "        'Scenario': scenario_name,\n",
                "        'Threshold': threshold,\n",
                "        'Precision': precision,\n",
                "        'Recall': recall,\n",
                "        'F1': f1,\n",
                "        'Flagged': tp + fp,\n",
                "        'True Positives': tp,\n",
                "        'False Positives': fp\n",
                "    })\n",
                "\n",
                "scenario_df = pd.DataFrame(scenario_results)\n",
                "\n",
                "print(\"\\nBusiness Scenario Analysis:\")\n",
                "print(\"=\"*100)\n",
                "print(scenario_df.to_string(index=False))\n",
                "print(\"\\n\" + \"=\"*100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Precision@K and Recall@K for Different K Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate Precision@K and Recall@K for various K values\n",
                "k_values = [50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
                "\n",
                "precision_at_k_values = []\n",
                "recall_at_k_values = []\n",
                "\n",
                "for k in k_values:\n",
                "    if k <= len(y_test):\n",
                "        prec_k = precision_at_k(y_test, y_pred_proba, k)\n",
                "        rec_k = recall_at_k(y_test, y_pred_proba, k)\n",
                "        precision_at_k_values.append(prec_k)\n",
                "        recall_at_k_values.append(rec_k)\n",
                "    else:\n",
                "        precision_at_k_values.append(np.nan)\n",
                "        recall_at_k_values.append(np.nan)\n",
                "\n",
                "# Create results table\n",
                "precision_recall_k_df = pd.DataFrame({\n",
                "    'K': k_values,\n",
                "    'Precision@K': precision_at_k_values,\n",
                "    'Recall@K': recall_at_k_values\n",
                "})\n",
                "\n",
                "print(\"\\nPrecision@K and Recall@K:\")\n",
                "print(\"=\"*70)\n",
                "print(precision_recall_k_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Precision@K and Recall@K\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Precision@K\n",
                "axes[0].plot(k_values, precision_at_k_values, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
                "axes[0].set_xlabel('K (Number of Top Predictions)', fontsize=12)\n",
                "axes[0].set_ylabel('Precision@K', fontsize=12)\n",
                "axes[0].set_title('Precision@K: Quality of Top Predictions', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xscale('log')\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Recall@K\n",
                "axes[1].plot(k_values, recall_at_k_values, marker='o', linewidth=2, markersize=8, color='orange')\n",
                "axes[1].set_xlabel('K (Number of Top Predictions)', fontsize=12)\n",
                "axes[1].set_ylabel('Recall@K', fontsize=12)\n",
                "axes[1].set_title('Recall@K: Coverage of Frauds', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xscale('log')\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../outputs/precision_recall_at_k_detailed.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. SHAP Explainability - Global Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create SHAP explainer\n",
                "# Use a sample for faster computation\n",
                "sample_size = min(1000, len(X_test))\n",
                "X_test_sample = X_test.sample(n=sample_size, random_state=42)\n",
                "\n",
                "print(f\"Creating SHAP explainer with {sample_size} samples...\")\n",
                "explainer = shap.TreeExplainer(model)\n",
                "shap_values = explainer.shap_values(X_test_sample)\n",
                "\n",
                "print(\"SHAP values computed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Global feature importance (mean absolute SHAP values)\n",
                "shap_importance = pd.DataFrame({\n",
                "    'feature': feature_cols,\n",
                "    'importance': np.abs(shap_values).mean(axis=0)\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "print(\"\\nTop 20 Features by SHAP Importance:\")\n",
                "print(\"=\"*70)\n",
                "print(shap_importance.head(20).to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SHAP summary plot (global feature importance)\n",
                "plt.figure(figsize=(12, 10))\n",
                "shap.summary_plot(shap_values, X_test_sample, max_display=20, show=False)\n",
                "plt.title('SHAP Feature Importance (Global)', fontsize=14, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.savefig('../outputs/shap_global_importance.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SHAP summary plot (feature impact)\n",
                "plt.figure(figsize=(12, 10))\n",
                "shap.summary_plot(shap_values, X_test_sample, plot_type='violin', max_display=20, show=False)\n",
                "plt.title('SHAP Feature Impact Distribution', fontsize=14, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.savefig('../outputs/shap_feature_impact.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. SHAP Explainability - Local Explanations\n",
                "\n",
                "Explaining individual fraud predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get top fraud predictions\n",
                "test_df_with_scores = test_df.copy()\n",
                "test_df_with_scores['fraud_score'] = y_pred_proba\n",
                "\n",
                "# Top 10 predicted frauds\n",
                "top_frauds = test_df_with_scores.nlargest(10, 'fraud_score')\n",
                "\n",
                "print(\"\\nTop 10 Predicted Frauds:\")\n",
                "print(\"=\"*100)\n",
                "print(top_frauds[['transaction_id', 'type', 'amount', 'isFraud', 'fraud_score']].to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SHAP waterfall plot for top predicted fraud\n",
                "# Get the index of the top fraud in our sample\n",
                "top_fraud_id = top_frauds.iloc[0]['transaction_id']\n",
                "\n",
                "# Find this transaction in our sample (if it exists)\n",
                "if top_fraud_id in X_test_sample.index:\n",
                "    idx_in_sample = X_test_sample.index.get_loc(top_fraud_id)\n",
                "    \n",
                "    print(f\"\\nExplaining transaction {top_fraud_id}:\")\n",
                "    print(f\"Fraud score: {y_pred_proba[top_fraud_id]:.4f}\")\n",
                "    print(f\"Actual label: {'FRAUD' if y_test.loc[top_fraud_id] == 1 else 'LEGITIMATE'}\")\n",
                "    \n",
                "    # Create waterfall plot\n",
                "    shap.waterfall_plot(shap.Explanation(\n",
                "        values=shap_values[idx_in_sample],\n",
                "        base_values=explainer.expected_value,\n",
                "        data=X_test_sample.iloc[idx_in_sample],\n",
                "        feature_names=feature_cols\n",
                "    ), max_display=15, show=False)\n",
                "    \n",
                "    plt.title(f'SHAP Explanation: Transaction {top_fraud_id}', fontsize=14, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('../outputs/shap_local_explanation_top_fraud.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "else:\n",
                "    print(f\"\\nTop fraud transaction {top_fraud_id} not in SHAP sample. Using first sample instead.\")\n",
                "    shap.waterfall_plot(shap.Explanation(\n",
                "        values=shap_values[0],\n",
                "        base_values=explainer.expected_value,\n",
                "        data=X_test_sample.iloc[0],\n",
                "        feature_names=feature_cols\n",
                "    ), max_display=15, show=False)\n",
                "    plt.title('SHAP Explanation: Sample Transaction', fontsize=14, fontweight='bold')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig('../outputs/shap_local_explanation_sample.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Generate Fraud Score Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create fraud score output\n",
                "fraud_score_df = pd.DataFrame({\n",
                "    'transaction_id': test_df['transaction_id'],\n",
                "    'fraud_score': y_pred_proba,\n",
                "    'isFraud': y_test\n",
                "})\n",
                "\n",
                "# Sort by fraud score (descending)\n",
                "fraud_score_df = fraud_score_df.sort_values('fraud_score', ascending=False).reset_index(drop=True)\n",
                "\n",
                "# Save to CSV\n",
                "output_path = '../outputs/fraud_score.csv'\n",
                "fraud_score_df.to_csv(output_path, index=False)\n",
                "\n",
                "print(f\"\\nFraud scores saved to: {output_path}\")\n",
                "print(f\"Total transactions: {len(fraud_score_df):,}\")\n",
                "print(\"\\nSample of top predictions:\")\n",
                "print(fraud_score_df.head(20))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze fraud score distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Fraud score distribution by actual label\n",
                "fraud_score_df[fraud_score_df['isFraud']==0]['fraud_score'].hist(\n",
                "    bins=50, alpha=0.5, label='Legitimate', ax=axes[0], color='green'\n",
                ")\n",
                "fraud_score_df[fraud_score_df['isFraud']==1]['fraud_score'].hist(\n",
                "    bins=50, alpha=0.5, label='Fraud', ax=axes[0], color='red'\n",
                ")\n",
                "axes[0].set_xlabel('Fraud Score', fontsize=12)\n",
                "axes[0].set_ylabel('Frequency', fontsize=12)\n",
                "axes[0].set_title('Fraud Score Distribution', fontsize=14, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].set_yscale('log')\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "# Cumulative fraud capture\n",
                "fraud_score_df['cumulative_frauds'] = fraud_score_df['isFraud'].cumsum()\n",
                "fraud_score_df['pct_transactions'] = (fraud_score_df.index + 1) / len(fraud_score_df) * 100\n",
                "fraud_score_df['pct_frauds_captured'] = fraud_score_df['cumulative_frauds'] / fraud_score_df['isFraud'].sum() * 100\n",
                "\n",
                "axes[1].plot(fraud_score_df['pct_transactions'], fraud_score_df['pct_frauds_captured'], \n",
                "             linewidth=2, color='steelblue')\n",
                "axes[1].plot([0, 100], [0, 100], 'r--', alpha=0.5, label='Random')\n",
                "axes[1].set_xlabel('% of Transactions Reviewed', fontsize=12)\n",
                "axes[1].set_ylabel('% of Frauds Captured', fontsize=12)\n",
                "axes[1].set_title('Cumulative Fraud Capture Curve', fontsize=14, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../outputs/fraud_score_analysis.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Production Recommendations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"PRODUCTION RECOMMENDATIONS\")\n",
                "print(\"=\"*100)\n",
                "\n",
                "print(\"\\n1. THRESHOLD SELECTION:\")\n",
                "print(\"   - For high-value transactions: Use threshold ≥ 0.9 (high precision)\")\n",
                "print(\"   - For automated blocking: Use threshold ≥ 0.7 (balanced)\")\n",
                "print(\"   - For manual review queue: Use threshold ≥ 0.3 (high recall)\")\n",
                "print(f\"   - Recommended: {optimal_threshold_f1:.3f} (maximizes F1 score)\")\n",
                "\n",
                "print(\"\\n2. RANKING APPROACH:\")\n",
                "print(\"   - Use fraud_score for ranking (don't just use binary threshold)\")\n",
                "print(\"   - Review top K transactions daily based on investigation capacity\")\n",
                "print(\"   - Precision@100 shows quality of top predictions\")\n",
                "\n",
                "print(\"\\n3. MONITORING:\")\n",
                "print(\"   - Track Precision@K and Recall@K over time\")\n",
                "print(\"   - Monitor fraud score distribution for drift\")\n",
                "print(\"   - Retrain model quarterly or when performance degrades\")\n",
                "\n",
                "print(\"\\n4. EXPLAINABILITY:\")\n",
                "print(\"   - Use SHAP values to explain flagged transactions\")\n",
                "print(\"   - Key fraud indicators: balance errors, transaction type, amount ratios\")\n",
                "print(\"   - Provide explanations to fraud investigators\")\n",
                "\n",
                "print(\"\\n5. MODEL LIMITATIONS:\")\n",
                "print(\"   - Model trained on historical patterns (may miss new fraud types)\")\n",
                "print(\"   - Balance errors are strong signal but not present in all frauds\")\n",
                "print(\"   - Consider ensemble with rule-based systems\")\n",
                "print(\"   - Regular retraining needed as fraud patterns evolve\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate final summary statistics\n",
                "from sklearn.metrics import roc_auc_score, average_precision_score\n",
                "\n",
                "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
                "pr_auc = average_precision_score(y_test, y_pred_proba)\n",
                "prec_100 = precision_at_k(y_test, y_pred_proba, 100)\n",
                "rec_1000 = recall_at_k(y_test, y_pred_proba, 1000)\n",
                "\n",
                "print(\"\\n\" + \"=\"*100)\n",
                "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
                "print(\"=\"*100)\n",
                "print(f\"\\nDataset:\")\n",
                "print(f\"  Test transactions: {len(y_test):,}\")\n",
                "print(f\"  Actual frauds: {y_test.sum():,} ({y_test.mean()*100:.4f}%)\")\n",
                "print(f\"\\nModel Performance:\")\n",
                "print(f\"  ROC-AUC:          {roc_auc:.4f}\")\n",
                "print(f\"  PR-AUC:           {pr_auc:.4f}\")\n",
                "print(f\"  Precision@100:    {prec_100:.4f}\")\n",
                "print(f\"  Recall@1000:      {rec_1000:.4f}\")\n",
                "print(f\"\\nBusiness Impact:\")\n",
                "print(f\"  By reviewing top 100 transactions, we catch {int(prec_100*100)} frauds\")\n",
                "print(f\"  By reviewing top 1000 transactions, we catch {rec_1000*100:.1f}% of all frauds\")\n",
                "print(\"\\n\" + \"=\"*100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"THRESHOLD TUNING AND EXPLAINABILITY COMPLETE!\")\n",
                "print(\"=\"*70)\n",
                "print(\"\\nAll outputs saved to ../outputs/\")\n",
                "print(\"  - fraud_score.csv\")\n",
                "print(\"  - threshold_optimization.png\")\n",
                "print(\"  - shap_global_importance.png\")\n",
                "print(\"  - shap_feature_impact.png\")\n",
                "print(\"  - fraud_score_analysis.png\")\n",
                "print(\"\\nProject complete! Review README.md for full documentation.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}